{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_oa1O8WRKNg"
      },
      "source": [
        "Assessment / Midterm: Machine Vision and Deep Learning\n",
        "1) Answer all questions\n",
        "\n",
        "2) This assessment is open-book. You are allowed to refer to any references including online materials, books, notes, codes, github links, etc\n",
        "\n",
        "3) Copy this notebook to your google drive (click FILE > save a copy in Drive)\n",
        "\n",
        "4) Upload the answer notebook & trained model (for Question 3) to your github.\n",
        "\n",
        "5) Submit the assessment by sharing the link to your github containing the answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D82uxjUeRQy4",
        "outputId": "4e28dc85-ed65-443f-bd9b-f438abd6b383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsX23tZGgz_4"
      },
      "source": [
        "**QUESTION 1 [15 marks]**\n",
        "\n",
        "a) Let *k*, *s* and *p* be 4, 2 and 1, respectively. What will be the shape of the output of the convolutional layer? (2 marks)\n",
        "\n",
        ">>import torch \\\n",
        "import torch.nn as nn \\\n",
        "input_img = torch.rand(1,3,10,10) \\\n",
        "layer = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=*k*, stride=*s*, padding=*p*)\n",
        "\n",
        "<span style=\"color:blue\">\n",
        "    answer: The shape of the output is [1,12,5,5]\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxuvPZD5gz_4"
      },
      "source": [
        "b) Batch Normalization (BN) normalizes the mean and standard deviation for each: (tick X for the correct answer) (2 marks)\n",
        "\n",
        " - [X] Individual feature map\n",
        " - [ ] Instance in the mini-batch\n",
        " - [ ] Spatial dimension    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKPcWtBZgz_4"
      },
      "source": [
        "c) Which one of the following is not an advantage of Batch Normalization (BN)? (tick X for the correct answer) (2 marks)\n",
        "\n",
        "- [ ] BN accelerates the training of deep neural networks and tackles the vanishing gradient problem.\n",
        "- [ ] For every input mini-batch, we calculate different statistics. This introduces some sort of regularization.\n",
        "- [ ] BN reduces the dependence of gradients on the scale of the parameters or of their initial values.\n",
        "- [X] BN needs a much slower learning rate for the total architecture to converge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyBUwmRDgz_5"
      },
      "source": [
        "d) Choose the correct statement. (tick X for the correct answer) (2 marks)\n",
        "\n",
        "- [ ] A larger kernel is preferred for information that resides globally, and a smaller kernel is preferred for information that is distributed locally.\n",
        "- [X] A larger kernel is preferred for information that resides locally, and a smaller kernel is preferred for information that is distributed globally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4tMGPYlgz_5"
      },
      "source": [
        "e) In the following network, how many learnable parameters (weights) are there? (2 marks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ6wN3lUd_qZ"
      },
      "source": [
        "model = nn.Sequential(\n",
        "        nn.Linear(3,20),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(20,2)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbYBt0iWgz_6"
      },
      "source": [
        "<span style=\"color:blue\">\n",
        "    answer: There are 100 learnable parameters.\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zatq_h5Lgz_6"
      },
      "source": [
        "f) Which of the following statements are true about deep neural networks? (tick X for the correct answer) (2 marks)\n",
        "\n",
        "- [X] Deep neural networks usually require a big amount of data to be properly trained.\n",
        "- [X] They can be fooled by adversarial/ noisy examples.\n",
        "- [X] They are difficult to interpret and understand.\n",
        "- [ ] They can still be subject to biases.\n",
        "- [ ] They fail to understand the context of the data that they are handling.\n",
        "- [ ] They perform very well on individual tasks but fail to generalize to many different tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX6J3xYvgz_7"
      },
      "source": [
        "g) Run the code in the next cell. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48Cz-mNHR8TW"
      },
      "source": [
        "g) Run the code in the next cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "hDTYh7J2R9-5",
        "outputId": "8c2a939d-d415-4979-9f30-ec54bc9bd3b3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# import numpy as np\n",
        "from sklearn.datasets import make_moons\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# generate synthetic data in 2D\n",
        "X, Y = make_moons(500, noise=0.2)\n",
        "\n",
        "# plot the data\n",
        "plt.scatter(X[:,0], X[:,1], c=Y)\n",
        "plt.title('2D Data')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87vSqBRtgz_7"
      },
      "source": [
        "Can this data be correctly separable using the classifier below? Why? (3 marks)\n",
        "\n",
        "model = nn.Sequential( \\\n",
        "    nn.Linear(n_input_dim, n_output), \\\n",
        "    nn.Sigmoid() \\\n",
        "    )\n",
        "    \n",
        "<span style=\"color:blue\">\n",
        "    answer: Yes, because the data is separated by having 2 values where 1 pass as the output and another 1 not to pass as the output. \n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjRauIpz8zun"
      },
      "source": [
        "**QUESTION 2 [50 marks]** \n",
        "\n",
        "The COVID-19 pandemic continues to have a devastating effect on the health and well-being of the global population. A critical step in the fight against COVID-19 is effective screening of infected patients, with one of the key screening approaches being radiology examination using chest radiography. As a deep learning engineer, you are tasked to classify a dataset of X-ray images as either \"normal\", \"covid\" or \"viral pneumonia\". Use the chest X-ray dataset publicly available at https://www.kaggle.com/datasets/pranavraikokte/covid19-image-dataset to answer the following questions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzzvkxpn8zuo"
      },
      "source": [
        "a) Train a feedforward neural network to solve the classification problem. Please include: **(10 marks)**\n",
        "\n",
        "    1) The dataloader to load the train and test datasets.\n",
        "\n",
        "    2) The model definition (either using sequential method OR pytorch class method).\n",
        "\n",
        "    3) The training loop.\n",
        "\n",
        "    4) Output the mean accuracy for the whole testing dataset.\n",
        "    \n",
        "    5) The hyperparameters are:\n",
        "        i  - architecture: 2 hidden layers\n",
        "        ii - input size  : 32x32 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36N2X2La-3U3",
        "outputId": "d3e28f14-711b-4708-dad5-c05f61a39093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number class available: 3\n"
          ]
        }
      ],
      "source": [
        "###############################################\n",
        "###############YOUR CODES HERE ################\n",
        "###############################################\n",
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import numpy\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "    transforms.Resize((32,32)),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Load the Data\n",
        "\n",
        "# Set train and valid directory paths\n",
        "dataset = '/content/drive/MyDrive/Covid19DataSet'\n",
        "\n",
        "train_directory = os.path.join(dataset, 'train')\n",
        "test_directory = os.path.join(dataset, 'test')\n",
        "\n",
        "# Batch size\n",
        "batchSize = 8\n",
        "\n",
        "# Number of classes\n",
        "num_classes = len(os.listdir(train_directory))\n",
        "print(\"number class available: \"+ str(num_classes))\n",
        "\n",
        "# Load Data from folders\n",
        "data = {\n",
        "    'train': datasets.ImageFolder(root=train_directory, transform=transform),\n",
        "\n",
        "    'test': datasets.ImageFolder(root=test_directory, transform=transform)\n",
        "}\n",
        "classes = ('Covid', 'Normal', 'Viral_Pneumonia')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxBbWbxxLAMu"
      },
      "outputs": [],
      "source": [
        "# 1) dataloader\n",
        "train_data_size = len(data['train'])\n",
        "test_data_size = len(data['test'])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(data['train'], batch_size=batchSize,\n",
        "                                          shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(data['test'], batch_size=batchSize,\n",
        "                                          shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxlfrO59NcuZ"
      },
      "outputs": [],
      "source": [
        "# 2) model definition\n",
        "class myModel(nn.Module): \n",
        "    def __init__(self): \n",
        "        super(myModel, self).__init__() \n",
        "        self.linear1 = nn.Linear(3072, 128) \n",
        "        self.linear2 = nn.Linear(128, 64)\n",
        "        self.linear3 = nn.Linear(64, 3) \n",
        "        self.relu = nn.ReLU()\n",
        "  \n",
        "    def forward(self, x): \n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.relu(x)\n",
        "        o = self.linear3(x)\n",
        "\n",
        "        return o \n",
        "    \n",
        "model = myModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaNQhf8tNDNm",
        "outputId": "895dc884-f737-4cde-bfcd-f13dbe65d08c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 0.5002, Accuracy: 80.0797%, \n",
            "\t\tValidation : Loss : 0.5597, Accuracy: 80.3030%, Time: 142.7552s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 0.2578, Accuracy: 92.0319%, \n",
            "\t\tValidation : Loss : 0.4250, Accuracy: 86.3636%, Time: 13.5074s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 0.1726, Accuracy: 94.0239%, \n",
            "\t\tValidation : Loss : 0.4403, Accuracy: 83.3333%, Time: 14.7790s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 0.1483, Accuracy: 94.8207%, \n",
            "\t\tValidation : Loss : 0.4088, Accuracy: 87.8788%, Time: 13.4862s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.1030, Accuracy: 95.6175%, \n",
            "\t\tValidation : Loss : 0.4084, Accuracy: 87.8788%, Time: 13.6709s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.0780, Accuracy: 96.4143%, \n",
            "\t\tValidation : Loss : 0.3991, Accuracy: 86.3636%, Time: 13.6770s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.0306, Accuracy: 99.6016%, \n",
            "\t\tValidation : Loss : 0.3552, Accuracy: 90.9091%, Time: 13.5009s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.0118, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.3663, Accuracy: 89.3939%, Time: 13.3817s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.0148, Accuracy: 99.6016%, \n",
            "\t\tValidation : Loss : 0.3687, Accuracy: 90.9091%, Time: 13.6639s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.0141, Accuracy: 100.0000%, \n",
            "\t\tValidation : Loss : 0.4386, Accuracy: 90.9091%, Time: 13.6804s\n"
          ]
        }
      ],
      "source": [
        "# 3) Training loop\n",
        "\n",
        "# LOSS AND OPTIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# move the model to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "import time # to calculate training time\n",
        "\n",
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "    '''\n",
        "    Function to train and validate\n",
        "    Parameters\n",
        "        :param model: Model to train and validate\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "        :param optimizer: Optimizer for computing gradients\n",
        "        :param epochs: Number of epochs (default=25)\n",
        "  \n",
        "    Returns\n",
        "        model: Trained Model with best validation accuracy\n",
        "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "    '''\n",
        "    \n",
        "    start = time.time()\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        \n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            #reshape images so they can be fed to a nn.Linear()\n",
        "            inputs = inputs.view(inputs.size(0), -1)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "            \n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Compute the accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            \n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            \n",
        "            # Compute total accuracy in the whole batch and add to train_acc\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "            \n",
        "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "            \n",
        "        # Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop\n",
        "            for j, (inputs, labels) in enumerate(testloader):\n",
        "                inputs = inputs.to(device)\n",
        "                #reshape images so they can be fed to a nn.Linear()\n",
        "                inputs = inputs.view(inputs.size(0), -1)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "                # Convert correct_counts to float and then compute the mean\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                # Compute total accuracy in the whole batch and add to valid_acc\n",
        "                valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "            \n",
        "        # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_data_size \n",
        "        avg_train_acc = train_acc/train_data_size\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_test_loss = valid_loss/test_data_size \n",
        "        avg_test_acc = valid_acc/test_data_size\n",
        "\n",
        "        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n",
        "                \n",
        "        epoch_end = time.time()\n",
        "    \n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n",
        "        \n",
        "        # Save if the model has best accuracy till now\n",
        "        torch.save(model, 'cifar10_model_'+str(epoch)+'.pt')\n",
        "            \n",
        "    return model, history\n",
        "\n",
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model, criterion, optimizer, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Output the mean accuracy for the whole testing dataset.\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "#         images, labels = data\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        images = images.view(images.size(0), -1)\n",
        "        outputs = model(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        for label, prediction in zip(labels, predicted):\n",
        "          if label == prediction:\n",
        "            correct_pred[classes[label]] += 1\n",
        "          total_pred[classes[label]] += 1\n",
        "\n",
        "print(f'Accuracy of the network on the test images: {100 * correct // total} %')\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "metadata": {
        "id": "HHJnPIaVWKYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e111df-356b-4668-ce0c-90ec71cb7eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 90 %\n",
            "Accuracy for class: Covid is 92.3 %\n",
            "Accuracy for class: Normal is 80.0 %\n",
            "Accuracy for class: Viral_Pneumonia is 100.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW6A4Pmj8zuo"
      },
      "source": [
        "b) Create and train a ConvNet corresponding to the following architecture in Fig. 1 (with modifications of the input and final layers). Please include **(10 marks)**:\n",
        "\n",
        "    1) The dataloader to load the train and test datasets.\n",
        "\n",
        "    2) The model definition (either using sequential method OR pytorch class method).\n",
        "\n",
        "    3) Define your training loop.\n",
        "\n",
        "    4) Output the mean accuracy for the whole testing dataset.\n",
        "\n",
        "![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/LeNet.png)\n",
        "    \n",
        "                                Fig. 1: A convolutional neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oH7jVZJTqbL",
        "outputId": "46460693-0ac7-475f-ff43-73fbf8df0033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number class available: 3\n"
          ]
        }
      ],
      "source": [
        "###############################################\n",
        "###############YOUR CODES HERE ################\n",
        "###############################################\n",
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import numpy\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "    transforms.Resize((32,32)),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Load the Data\n",
        "\n",
        "# Set train and valid directory paths\n",
        "dataset = '/content/drive/MyDrive/Covid19DataSet'\n",
        "\n",
        "train_directory = os.path.join(dataset, 'train')\n",
        "test_directory = os.path.join(dataset, 'test')\n",
        "\n",
        "# Batch size\n",
        "batchSize = 8\n",
        "\n",
        "# Number of classes\n",
        "num_classes = len(os.listdir(train_directory))\n",
        "print(\"number class available: \"+ str(num_classes))\n",
        "\n",
        "# Load Data from folders\n",
        "data = {\n",
        "    'train': datasets.ImageFolder(root=train_directory, transform=transform),\n",
        "\n",
        "    'test': datasets.ImageFolder(root=test_directory, transform=transform)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc2N8TmPUG4h"
      },
      "outputs": [],
      "source": [
        "# 1) dataloader\n",
        "train_data_size = len(data['train'])\n",
        "test_data_size = len(data['test'])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(data['train'], batch_size=batchSize,\n",
        "                                          shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(data['test'], batch_size=batchSize,\n",
        "                                          shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxjgkymUUMrE"
      },
      "outputs": [],
      "source": [
        "# 2) model definition\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5, 1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(5*5*16, 120)\n",
        "        self.fc2 = nn.Linear(120, 3)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 5*5*16)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "model = CNN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcxyTKDeUSup",
        "outputId": "9cbc2f43-deb5-46c8-c022-0ad045479ed1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 0.8774, Accuracy: 58.9641%, \n",
            "\t\tValidation : Loss : 0.5871, Accuracy: 66.6667%, Time: 17.9682s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 0.3931, Accuracy: 84.8606%, \n",
            "\t\tValidation : Loss : 0.5173, Accuracy: 81.8182%, Time: 13.6149s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 0.2745, Accuracy: 89.2430%, \n",
            "\t\tValidation : Loss : 0.4306, Accuracy: 80.3030%, Time: 13.7277s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 0.2845, Accuracy: 87.6494%, \n",
            "\t\tValidation : Loss : 0.5866, Accuracy: 75.7576%, Time: 13.8241s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.2256, Accuracy: 92.8287%, \n",
            "\t\tValidation : Loss : 0.4193, Accuracy: 83.3333%, Time: 13.7233s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.1854, Accuracy: 92.8287%, \n",
            "\t\tValidation : Loss : 0.4664, Accuracy: 80.3030%, Time: 13.6098s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.1916, Accuracy: 92.4303%, \n",
            "\t\tValidation : Loss : 0.3890, Accuracy: 86.3636%, Time: 13.6671s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.1635, Accuracy: 93.2271%, \n",
            "\t\tValidation : Loss : 0.4251, Accuracy: 83.3333%, Time: 13.6145s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.1402, Accuracy: 94.8207%, \n",
            "\t\tValidation : Loss : 0.3765, Accuracy: 86.3636%, Time: 13.7683s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.1240, Accuracy: 96.0159%, \n",
            "\t\tValidation : Loss : 0.4486, Accuracy: 81.8182%, Time: 13.6966s\n"
          ]
        }
      ],
      "source": [
        "# 3) Training loop\n",
        "\n",
        "# LOSS AND OPTIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# move the model to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "import time # to calculate training time\n",
        "\n",
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "    '''\n",
        "    Function to train and validate\n",
        "    Parameters\n",
        "        :param model: Model to train and validate\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "        :param optimizer: Optimizer for computing gradients\n",
        "        :param epochs: Number of epochs (default=25)\n",
        "  \n",
        "    Returns\n",
        "        model: Trained Model with best validation accuracy\n",
        "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "    '''\n",
        "    \n",
        "    start = time.time()\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        \n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "            \n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Compute the accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            \n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            \n",
        "            # Compute total accuracy in the whole batch and add to train_acc\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "            \n",
        "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "            \n",
        "        # Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop\n",
        "            for j, (inputs, labels) in enumerate(testloader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "                # Convert correct_counts to float and then compute the mean\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                # Compute total accuracy in the whole batch and add to valid_acc\n",
        "                valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "            \n",
        "        # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_data_size \n",
        "        avg_train_acc = train_acc/train_data_size\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_test_loss = valid_loss/test_data_size \n",
        "        avg_test_acc = valid_acc/test_data_size\n",
        "\n",
        "        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n",
        "                \n",
        "        epoch_end = time.time()\n",
        "    \n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n",
        "        \n",
        "        # Save if the model has best accuracy till now\n",
        "        torch.save(model, 'cifar10_model_'+str(epoch)+'.pt')\n",
        "            \n",
        "    return model, history\n",
        "\n",
        "    \n",
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model, criterion, optimizer, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Output the mean accuracy for the whole testing dataset.\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "#         images, labels = data\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = model(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        for label, prediction in zip(labels, predicted):\n",
        "          if label == prediction:\n",
        "            correct_pred[classes[label]] += 1\n",
        "          total_pred[classes[label]] += 1\n",
        "\n",
        "print(f'Accuracy of the network on the test images: {100 * correct // total} %')\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "metadata": {
        "id": "czFWm6P7XnV3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d5885c3-e426-410e-dc8f-9d17d738ad90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 81 %\n",
            "Accuracy for class: Covid is 88.5 %\n",
            "Accuracy for class: Normal is 65.0 %\n",
            "Accuracy for class: Viral_Pneumonia is 90.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in trainloader:\n",
        "#         images, labels = data\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = model(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        for label, prediction in zip(labels, predicted):\n",
        "          if label == prediction:\n",
        "            correct_pred[classes[label]] += 1\n",
        "          total_pred[classes[label]] += 1\n",
        "\n",
        "print(f'Accuracy of the network on the train images: {100 * correct // total} %')\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjbbIURGuSsg",
        "outputId": "6e7fc4bd-6b49-475a-9042-84605faac1a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the train images: 96 %\n",
            "Accuracy for class: Covid is 99.1 %\n",
            "Accuracy for class: Normal is 90.0 %\n",
            "Accuracy for class: Viral_Pneumonia is 98.6 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output\n",
        "# 5. Analyze the loss curve\n",
        "\n",
        "history = np.array(history)\n",
        "plt.plot(history[:,2:4])\n",
        "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0,1)\n",
        "# plt.savefig('cifar10_accuracy_curve.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "gxmaMOI-uN90",
        "outputId": "0fddccb9-2513-4f38-cfc7-2302e14795aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzNfOwTSgz_8"
      },
      "source": [
        "c) Based on Question 2 b), answer the following questions: \n",
        "    \n",
        "    i) Is the model overfitting? Why? (4 marks)\n",
        "      <font color='blue'>YES. Because the accuracy of train and validation dataset has a gap of 10%</font>\n",
        "    \n",
        "    ii) Propose two methods to reduce the effect of overfitting. (4 marks)\n",
        "   <font color='blue'>Two methods that I propose are: (1) data augmentation (2)using dropout and batch normalization</font>\n",
        "   \n",
        "    iii) Implement the two proposed methods in Q2 c) ii) in the next cell and show that the overfitting has been reduced. (8 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UeYFijRjZwQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b254b5-51cd-4c97-f634-78b0c2d69383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number class available: 3\n"
          ]
        }
      ],
      "source": [
        "###############################################\n",
        "###############YOUR CODES HERE ################\n",
        "###############################################\n",
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import numpy\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((32,32)),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'valid': transforms.Compose([\n",
        "        transforms.Resize((32,32)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Load the Data\n",
        "\n",
        "# Set train and valid directory paths\n",
        "dataset = '/content/drive/MyDrive/Covid19DataSet'\n",
        "\n",
        "train_directory = os.path.join(dataset, 'train')\n",
        "test_directory = os.path.join(dataset, 'test')\n",
        "\n",
        "# Batch size\n",
        "batchSize = 8\n",
        "\n",
        "# Number of classes\n",
        "num_classes = len(os.listdir(train_directory))\n",
        "print(\"number class available: \"+ str(num_classes))\n",
        "\n",
        "# Load Data from folders\n",
        "data = {\n",
        "    'train': datasets.ImageFolder(root=train_directory, transform=transforms['train']),\n",
        "\n",
        "    'test': datasets.ImageFolder(root=test_directory, transform=transforms['valid'])\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzWTjCPjnUq1"
      },
      "outputs": [],
      "source": [
        "# 1) dataloader\n",
        "train_data_size = len(data['train'])\n",
        "test_data_size = len(data['test'])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(data['train'], batch_size=batchSize,\n",
        "                                          shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(data['test'], batch_size=batchSize,\n",
        "                                          shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hnzfkmu1nXnj"
      },
      "outputs": [],
      "source": [
        "# 2) model definition\n",
        "class CNNImproved(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNImproved, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5, 1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(5*5*16, 120)\n",
        "        self.fc2 = nn.Linear(120, 3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.batchnorm = nn.BatchNorm2d(6)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.batchnorm(x)\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(-1, 5*5*16)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "model = CNNImproved()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSHi4WB1nbS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa415374-e89a-499c-ca74-59dfedc33555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 0.7165, Accuracy: 70.9163%, \n",
            "\t\tValidation : Loss : 0.4782, Accuracy: 78.7879%, Time: 15.4374s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 0.3234, Accuracy: 86.8526%, \n",
            "\t\tValidation : Loss : 0.4349, Accuracy: 78.7879%, Time: 13.8150s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 0.2751, Accuracy: 88.8446%, \n",
            "\t\tValidation : Loss : 0.3821, Accuracy: 80.3030%, Time: 13.6412s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 0.2258, Accuracy: 90.4382%, \n",
            "\t\tValidation : Loss : 0.3588, Accuracy: 81.8182%, Time: 13.4538s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.2191, Accuracy: 91.6335%, \n",
            "\t\tValidation : Loss : 0.3820, Accuracy: 81.8182%, Time: 13.7292s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.2076, Accuracy: 92.8287%, \n",
            "\t\tValidation : Loss : 0.4141, Accuracy: 78.7879%, Time: 13.6153s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.1924, Accuracy: 92.8287%, \n",
            "\t\tValidation : Loss : 0.2778, Accuracy: 90.9091%, Time: 13.7316s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.1598, Accuracy: 93.2271%, \n",
            "\t\tValidation : Loss : 0.3092, Accuracy: 86.3636%, Time: 13.4278s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.1902, Accuracy: 92.8287%, \n",
            "\t\tValidation : Loss : 0.3378, Accuracy: 86.3636%, Time: 13.5070s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.1808, Accuracy: 92.8287%, \n",
            "\t\tValidation : Loss : 0.2822, Accuracy: 87.8788%, Time: 13.8005s\n"
          ]
        }
      ],
      "source": [
        "# 3) Training loop\n",
        "\n",
        "# LOSS AND OPTIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# move the model to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "import time # to calculate training time\n",
        "\n",
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "    '''\n",
        "    Function to train and validate\n",
        "    Parameters\n",
        "        :param model: Model to train and validate\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "        :param optimizer: Optimizer for computing gradients\n",
        "        :param epochs: Number of epochs (default=25)\n",
        "  \n",
        "    Returns\n",
        "        model: Trained Model with best validation accuracy\n",
        "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "    '''\n",
        "    \n",
        "    start = time.time()\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        \n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "            \n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Compute the accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            \n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            \n",
        "            # Compute total accuracy in the whole batch and add to train_acc\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "            \n",
        "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "            \n",
        "        # Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop\n",
        "            for j, (inputs, labels) in enumerate(testloader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "                # Convert correct_counts to float and then compute the mean\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                # Compute total accuracy in the whole batch and add to valid_acc\n",
        "                valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "            \n",
        "        # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_data_size \n",
        "        avg_train_acc = train_acc/train_data_size\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_test_loss = valid_loss/test_data_size \n",
        "        avg_test_acc = valid_acc/test_data_size\n",
        "\n",
        "        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n",
        "                \n",
        "        epoch_end = time.time()\n",
        "    \n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n",
        "        \n",
        "        # Save if the model has best accuracy till now\n",
        "        torch.save(model, 'cifar10_model_'+str(epoch)+'.pt')\n",
        "            \n",
        "    return model, history\n",
        "\n",
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model, criterion, optimizer, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Output the mean accuracy for the whole testing dataset.\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "#         images, labels = data\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = model(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        for label, prediction in zip(labels, predicted):\n",
        "          if label == prediction:\n",
        "            correct_pred[classes[label]] += 1\n",
        "          total_pred[classes[label]] += 1\n",
        "\n",
        "print(f'Accuracy of the network on the test images: {100 * correct // total} %')\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b375f0dd-990c-493f-9031-a85ee4ec4087",
        "id": "L22_hAIKONNs"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 87 %\n",
            "Accuracy for class: Covid is 96.2 %\n",
            "Accuracy for class: Normal is 85.0 %\n",
            "Accuracy for class: Viral_Pneumonia is 80.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in trainloader:\n",
        "#         images, labels = data\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = model(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        for label, prediction in zip(labels, predicted):\n",
        "          if label == prediction:\n",
        "            correct_pred[classes[label]] += 1\n",
        "          total_pred[classes[label]] += 1\n",
        "\n",
        "print(f'Accuracy of the network on the train images: {100 * correct // total} %')\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c4316f9-99ac-4c46-e838-351f5c56340f",
        "id": "AbmXyA2TONNt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the train images: 90 %\n",
            "Accuracy for class: Covid is 100.0 %\n",
            "Accuracy for class: Normal is 72.9 %\n",
            "Accuracy for class: Viral_Pneumonia is 91.4 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output\n",
        "# 5. Analyze the loss curve\n",
        "\n",
        "history = np.array(history)\n",
        "plt.plot(history[:,2:4])\n",
        "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0,1)\n",
        "# plt.savefig('cifar10_accuracy_curve.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "759ee21f-381b-4f68-c93b-309cfc22b003",
        "id": "uAxnSd9eONNu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1Zjsjxq8zuq"
      },
      "source": [
        "d) Replace your defined ConvNet in b) with a pre-trained model. Then, proceed with a transfer learning and finetune the model for the COVID-19 dataset. **(10 marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_pXFJUdpI_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a525b2dd-e4f2-4117-e68f-46f7b7a24ef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number class available: 3\n"
          ]
        }
      ],
      "source": [
        "###############################################\n",
        "###############YOUR CODES HERE ################\n",
        "###############################################\n",
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import numpy\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((32,32)),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'valid': transforms.Compose([\n",
        "        transforms.Resize((32,32)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Load the Data\n",
        "\n",
        "# Set train and valid directory paths\n",
        "dataset = '/content/drive/MyDrive/Covid19DataSet'\n",
        "\n",
        "train_directory = os.path.join(dataset, 'train')\n",
        "test_directory = os.path.join(dataset, 'test')\n",
        "\n",
        "# Batch size\n",
        "batchSize = 8\n",
        "\n",
        "# Number of classes\n",
        "num_classes = len(os.listdir(train_directory))\n",
        "print(\"number class available: \"+ str(num_classes))\n",
        "\n",
        "# Load Data from folders\n",
        "data = {\n",
        "    'train': datasets.ImageFolder(root=train_directory, transform=transforms['train']),\n",
        "\n",
        "    'test': datasets.ImageFolder(root=test_directory, transform=transforms['valid'])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CH6qwJK5pRgx"
      },
      "outputs": [],
      "source": [
        "# 1) dataloader\n",
        "train_data_size = len(data['train'])\n",
        "test_data_size = len(data['test'])\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(data['train'], batch_size=batchSize,\n",
        "                                          shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(data['test'], batch_size=batchSize,\n",
        "                                          shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSlyo1VQpWlN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "f4e43933d5ad476583d7eee0d9dc09be",
            "4d9ded1fd76f44fea1408fdf65be08f3",
            "0373f147e91a4b778bed454de012a248",
            "167ea6a856244d3b9907242c081c9bf1",
            "4a12f6f596bb49cfa501365fdea97c70",
            "d76347c85b6c4dc2963f6c274a279772",
            "5964e9a766db4c70b872205b5403b45d",
            "14ba82b2989847939afe4128ee9c2672",
            "d202564baa6648898296e66ffea54b90",
            "3488e2b0f21f44868b16fc1909ef8ab4",
            "a3710d75470c45d19a47db5d0cff320a"
          ]
        },
        "outputId": "658269fd-68f4-439c-ffde-2592378e995b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/googlenet-1378be20.pth\" to /root/.cache/torch/hub/checkpoints/googlenet-1378be20.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/49.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4e43933d5ad476583d7eee0d9dc09be"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 2) model definition\n",
        "model = models.googlenet(pretrained=True)\n",
        "model.fc = nn.Linear(in_features=1024, out_features=3, bias=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCTgseMCbOiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "834fa990-7968-4297-bcba-8c53c7a74232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 1.1381, Accuracy: 33.0677%, \n",
            "\t\tValidation : Loss : 1.1126, Accuracy: 34.8485%, Time: 15.4380s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 1.0934, Accuracy: 38.2470%, \n",
            "\t\tValidation : Loss : 1.0301, Accuracy: 48.4848%, Time: 17.1977s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 1.0540, Accuracy: 46.2151%, \n",
            "\t\tValidation : Loss : 1.0053, Accuracy: 43.9394%, Time: 15.1272s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 1.0145, Accuracy: 46.2151%, \n",
            "\t\tValidation : Loss : 0.9493, Accuracy: 56.0606%, Time: 15.1328s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.9424, Accuracy: 58.5657%, \n",
            "\t\tValidation : Loss : 0.9018, Accuracy: 59.0909%, Time: 16.2863s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.9103, Accuracy: 57.7689%, \n",
            "\t\tValidation : Loss : 0.8983, Accuracy: 57.5758%, Time: 16.1267s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.8721, Accuracy: 68.5259%, \n",
            "\t\tValidation : Loss : 0.7966, Accuracy: 66.6667%, Time: 15.1922s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.7944, Accuracy: 69.7211%, \n",
            "\t\tValidation : Loss : 0.6673, Accuracy: 71.2121%, Time: 15.1463s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.7282, Accuracy: 76.8924%, \n",
            "\t\tValidation : Loss : 0.6394, Accuracy: 72.7273%, Time: 15.2741s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.7303, Accuracy: 70.5179%, \n",
            "\t\tValidation : Loss : 0.6962, Accuracy: 66.6667%, Time: 15.2358s\n"
          ]
        }
      ],
      "source": [
        "# 3) Training loop\n",
        "\n",
        "# LOSS AND OPTIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.RAdam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, foreach=None)\n",
        "\n",
        "# move the model to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "import time # to calculate training time\n",
        "\n",
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "    '''\n",
        "    Function to train and validate\n",
        "    Parameters\n",
        "        :param model: Model to train and validate\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "        :param optimizer: Optimizer for computing gradients\n",
        "        :param epochs: Number of epochs (default=25)\n",
        "  \n",
        "    Returns\n",
        "        model: Trained Model with best validation accuracy\n",
        "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "    '''\n",
        "    \n",
        "    start = time.time()\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        \n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "            \n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Compute the accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            \n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            \n",
        "            # Compute total accuracy in the whole batch and add to train_acc\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "            \n",
        "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "            \n",
        "        # Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop\n",
        "            for j, (inputs, labels) in enumerate(testloader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "                # Convert correct_counts to float and then compute the mean\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                # Compute total accuracy in the whole batch and add to valid_acc\n",
        "                valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "            \n",
        "        # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_data_size \n",
        "        avg_train_acc = train_acc/train_data_size\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_test_loss = valid_loss/test_data_size \n",
        "        avg_test_acc = valid_acc/test_data_size\n",
        "\n",
        "        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n",
        "                \n",
        "        epoch_end = time.time()\n",
        "    \n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n",
        "        \n",
        "        # Save if the model has best accuracy till now\n",
        "        torch.save(model, 'cifar10_model_'+str(epoch)+'.pt')\n",
        "            \n",
        "    return model, history\n",
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model, criterion, optimizer, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Output the mean accuracy for the whole testing dataset.\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "#         images, labels = data\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = model(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        for label, prediction in zip(labels, predicted):\n",
        "          if label == prediction:\n",
        "            correct_pred[classes[label]] += 1\n",
        "          total_pred[classes[label]] += 1\n",
        "\n",
        "print(f'Accuracy of the network on the test images: {100 * correct // total} %')\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06b49ce0-dbd7-4693-c822-3f9fbca92d67",
        "id": "fNVzXbX1OaNE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 66 %\n",
            "Accuracy for class: Covid is 84.6 %\n",
            "Accuracy for class: Normal is 55.0 %\n",
            "Accuracy for class: Viral_Pneumonia is 55.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in trainloader:\n",
        "#         images, labels = data\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = model(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        for label, prediction in zip(labels, predicted):\n",
        "          if label == prediction:\n",
        "            correct_pred[classes[label]] += 1\n",
        "          total_pred[classes[label]] += 1\n",
        "\n",
        "print(f'Accuracy of the network on the train images: {100 * correct // total} %')\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee47a788-b339-42e8-cdc4-79827a5699e6",
        "id": "-0jCbFAPOaNF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the train images: 77 %\n",
            "Accuracy for class: Covid is 87.4 %\n",
            "Accuracy for class: Normal is 71.4 %\n",
            "Accuracy for class: Viral_Pneumonia is 68.6 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output\n",
        "# 5. Analyze the loss curve\n",
        "\n",
        "history = np.array(history)\n",
        "plt.plot(history[:,2:4])\n",
        "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0,1)\n",
        "# plt.savefig('cifar10_accuracy_curve.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "dc161f56-17b0-4091-9605-7585601dd420",
        "id": "JpeBQj4NOaNG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzPPxsCX8zus"
      },
      "source": [
        "e) Do you see any accuracy improvement? Whether it is a \"yes\" or \"no\", discuss the possible reasons contributing to the accuracy improvement/ unimprovement. **(4 marks)**\n",
        "\n",
        "<span style=\"color:blue\">\n",
        "    Yes. The improvement of the accuracy is contributed by the pretrained model and it is also due to the batch normalization, dropout and data augmentation. Moreover, the accuracy also can be improved by increasing the size of dataset to get more familiarize. Here, the overfitting issue will also be countered. </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCy3b5888zut"
      },
      "source": [
        "**QUESTION 3** **[15 marks]**\n",
        "\n",
        "In a machine vision project, you decide to design a door access control system based on hand gestures. Only those who shows the correct hand gesture will be granted the access. There are three gestures that are recognized as correct access passcode which are \"shaka\", \"peace\" and \"thumbs up\", as depicted in Fig. 2.\n",
        "\n",
        "![pic](https://simplyorganizedhi.com/media/catalog/product/1/6/169500.jpg)\n",
        "\n",
        "                                Fig. 2 Shaka,peace, and thumbs up\n",
        "\n",
        "\n",
        "Using the end-to-end object detection pipeline that you have learned, develop your own hand gesture detector so that it can be incorporate to a door access control system.\n",
        "\n",
        "Deliverables for this question are:\n",
        "\n",
        "- the model file. Change the name to <your_name>.pt file (e.g. hasan.pt).\n",
        "\n",
        "- 5 marks for plausible detection of each gesture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6hyX9Tup6g9",
        "outputId": "8c03d584-03c7-45cc-a115-187dd8f8415e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 14474, done.\u001b[K\n",
            "remote: Total 14474 (delta 0), reused 0 (delta 0), pack-reused 14474\u001b[K\n",
            "Receiving objects: 100% (14474/14474), 13.62 MiB | 12.42 MiB/s, done.\n",
            "Resolving deltas: 100% (9978/9978), done.\n",
            "/content/yolov5\n",
            "\u001b[K     || 182 kB 25.7 MB/s \n",
            "\u001b[K     || 62 kB 1.5 MB/s \n",
            "\u001b[K     || 1.6 MB 76.2 MB/s \n",
            "\u001b[K     || 42 kB 820 kB/s \n",
            "\u001b[K     || 67 kB 6.9 MB/s \n",
            "\u001b[K     || 54 kB 3.1 MB/s \n",
            "\u001b[K     || 178 kB 69.3 MB/s \n",
            "\u001b[K     || 145 kB 74.0 MB/s \n",
            "\u001b[K     || 138 kB 81.3 MB/s \n",
            "\u001b[K     || 62 kB 1.6 MB/s \n",
            "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Setup complete. Using torch 1.13.0+cu116 (Tesla T4)\n"
          ]
        }
      ],
      "source": [
        "#clone YOLOv5 and \n",
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt # install dependencies\n",
        "%pip install -q roboflow\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u6TFW4I3krK",
        "outputId": "3a763d1e-e653-4d98-80ba-e374dba0c88f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.8/dist-packages (0.2.21)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.8/dist-packages (from roboflow) (2.4.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from roboflow) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from roboflow) (1.21.6)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.8/dist-packages (from roboflow) (0.10.1)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.8/dist-packages (from roboflow) (3.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from roboflow) (2.28.1)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.8/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: certifi==2021.5.30 in /usr/local/lib/python3.8/dist-packages (from roboflow) (2021.5.30)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from roboflow) (6.0)\n",
            "Requirement already satisfied: urllib3==1.26.6 in /usr/local/lib/python3.8/dist-packages (from roboflow) (1.26.6)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.8/dist-packages (from roboflow) (0.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from roboflow) (1.15.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.8/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: opencv-python-headless>=4.5.1.48 in /usr/local/lib/python3.8/dist-packages (from roboflow) (4.6.0.66)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.8/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.8/dist-packages (from roboflow) (4.64.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.8/dist-packages (from roboflow) (1.4.4)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.8/dist-packages (from roboflow) (0.7)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from roboflow) (7.1.2)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->roboflow) (2.1.1)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in hans-2 to yolov5pytorch: 100% [13044788 / 13044788] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to hans-2 in yolov5pytorch:: 100%|| 609/609 [00:00<00:00, 1310.07it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"8Nf28czlvaFxTQJDX15g\")\n",
        "project = rf.workspace(\"school-hvqrt\").project(\"hans-dhzvi\")\n",
        "dataset = project.version(2).download(\"yolov5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 640 --batch 4 --epochs 100 --data {dataset.location}/data.yaml --weights yolov5s.pt --cache --project /content/drive/MyDrive --name alwani"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwEDXv9EqdCh",
        "outputId": "b923248b-ca3f-482a-a981-446e380791dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/content/yolov5/hans-2/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=4, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=/content/drive/MyDrive, name=alwani, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 \n",
            "YOLOv5  v7.0-32-g357cde9 Python-3.8.16 torch-1.13.0+cu116 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5  in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5  runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /content/drive/MyDrive', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 58.0MB/s]\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 247MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=3\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     21576  models.yolo.Detect                      [3, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7027720 parameters, 7027720 gradients, 16.0 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/yolov5/hans-2/train/labels... 210 images, 0 backgrounds, 0 corrupt: 100% 210/210 [00:00<00:00, 1649.83it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/yolov5/hans-2/train/labels.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100% 210/210 [00:01<00:00, 186.43it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolov5/hans-2/valid/labels... 90 images, 0 backgrounds, 0 corrupt: 100% 90/90 [00:00<00:00, 351.08it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/yolov5/hans-2/valid/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100% 90/90 [00:00<00:00, 191.89it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.59 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset \n",
            "Plotting labels to /content/drive/MyDrive/alwani/labels.jpg... \n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/alwani\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       0/99     0.988G     0.1064    0.02794    0.04012          3        640: 100% 53/53 [00:08<00:00,  5.90it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:02<00:00,  4.66it/s]\n",
            "                   all         90         90    0.00144      0.451    0.00305    0.00077\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       1/99      1.19G    0.08685    0.02331    0.03488          4        640: 100% 53/53 [00:06<00:00,  7.99it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 10.56it/s]\n",
            "                   all         90         90    0.00254      0.754     0.0331    0.00778\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       2/99       1.2G    0.07969    0.02222    0.03073          5        640: 100% 53/53 [00:06<00:00,  8.25it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00,  9.52it/s]\n",
            "                   all         90         90      0.399      0.244      0.147     0.0298\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       3/99       1.2G    0.07367     0.0198    0.02796          1        640: 100% 53/53 [00:06<00:00,  8.09it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.65it/s]\n",
            "                   all         90         90      0.385      0.141      0.116     0.0247\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       4/99       1.2G     0.0719    0.02047    0.02477          7        640: 100% 53/53 [00:06<00:00,  8.53it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.15it/s]\n",
            "                   all         90         90       0.69     0.0769      0.107     0.0227\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       5/99       1.2G    0.06666    0.01853    0.02314          3        640: 100% 53/53 [00:06<00:00,  8.34it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00,  8.42it/s]\n",
            "                   all         90         90      0.769      0.218      0.362      0.119\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       6/99       1.2G    0.06588    0.01938    0.02248          6        640: 100% 53/53 [00:09<00:00,  5.30it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.44it/s]\n",
            "                   all         90         90      0.634       0.39      0.478      0.151\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       7/99       1.2G    0.06357    0.01863    0.02099          5        640: 100% 53/53 [00:06<00:00,  8.00it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.54it/s]\n",
            "                   all         90         90      0.701      0.383      0.477      0.175\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       8/99       1.2G    0.06053    0.01671    0.01942          3        640: 100% 53/53 [00:06<00:00,  8.51it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00,  9.83it/s]\n",
            "                   all         90         90      0.428      0.566      0.449      0.144\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       9/99       1.2G    0.06077    0.01552    0.01925          3        640: 100% 53/53 [00:06<00:00,  8.19it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 10.84it/s]\n",
            "                   all         90         90      0.462      0.592      0.543      0.202\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      10/99       1.2G    0.05991     0.0149    0.01643          4        640: 100% 53/53 [00:06<00:00,  8.55it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00,  9.78it/s]\n",
            "                   all         90         90      0.606      0.647       0.67      0.266\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      11/99       1.2G    0.05612    0.01309    0.01518          5        640: 100% 53/53 [00:06<00:00,  8.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.83it/s]\n",
            "                   all         90         90      0.531      0.536      0.548       0.21\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      12/99       1.2G    0.05645    0.01302     0.0135          2        640: 100% 53/53 [00:06<00:00,  8.44it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.87it/s]\n",
            "                   all         90         90      0.376      0.602      0.495      0.184\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      13/99       1.2G    0.05739    0.01386     0.0121          2        640: 100% 53/53 [00:06<00:00,  8.31it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.01it/s]\n",
            "                   all         90         90      0.572      0.674      0.612      0.244\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      14/99       1.2G    0.05394     0.0127    0.01038          5        640: 100% 53/53 [00:06<00:00,  8.37it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.88it/s]\n",
            "                   all         90         90      0.609       0.63       0.59      0.231\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      15/99       1.2G    0.05335    0.01267   0.009314          4        640: 100% 53/53 [00:06<00:00,  7.98it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 10.76it/s]\n",
            "                   all         90         90      0.645      0.718      0.664      0.298\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      16/99       1.2G    0.05298    0.01218   0.009581          2        640: 100% 53/53 [00:06<00:00,  8.46it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00,  9.70it/s]\n",
            "                   all         90         90      0.632      0.735       0.72      0.251\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      17/99       1.2G    0.04864    0.01254   0.008097          3        640: 100% 53/53 [00:06<00:00,  8.37it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 10.77it/s]\n",
            "                   all         90         90      0.634      0.679      0.698      0.287\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      18/99       1.2G    0.05135    0.01237   0.008472          2        640: 100% 53/53 [00:06<00:00,  8.55it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 10.64it/s]\n",
            "                   all         90         90      0.757      0.772      0.837      0.323\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      19/99       1.2G    0.05092    0.01211   0.007974          5        640: 100% 53/53 [00:06<00:00,  8.00it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.98it/s]\n",
            "                   all         90         90      0.709      0.684      0.716       0.25\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      20/99       1.2G    0.04989    0.01204   0.007446          4        640: 100% 53/53 [00:06<00:00,  8.54it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.68it/s]\n",
            "                   all         90         90      0.821      0.735      0.818      0.345\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      21/99       1.2G    0.04752     0.0106   0.006082          3        640: 100% 53/53 [00:06<00:00,  7.82it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.02it/s]\n",
            "                   all         90         90      0.729      0.852      0.842      0.362\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      22/99       1.2G    0.04853    0.01141   0.008031          3        640: 100% 53/53 [00:06<00:00,  8.23it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00,  9.59it/s]\n",
            "                   all         90         90      0.743      0.723      0.818      0.355\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      23/99       1.2G    0.04758     0.0109   0.006174          3        640: 100% 53/53 [00:06<00:00,  8.39it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 10.83it/s]\n",
            "                   all         90         90      0.848      0.885      0.904      0.407\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      24/99       1.2G    0.04493    0.01096   0.006732          3        640: 100% 53/53 [00:06<00:00,  8.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00,  9.94it/s]\n",
            "                   all         90         90      0.901      0.882      0.947      0.432\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      25/99       1.2G    0.04585    0.01123   0.006707          2        640: 100% 53/53 [00:06<00:00,  8.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.33it/s]\n",
            "                   all         90         90      0.865      0.846       0.91      0.415\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      26/99       1.2G    0.04725     0.0109    0.00558          5        640: 100% 53/53 [00:06<00:00,  8.27it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 10.01it/s]\n",
            "                   all         90         90       0.78      0.826      0.815      0.315\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      27/99       1.2G    0.04579    0.01162   0.006225          1        640: 100% 53/53 [00:06<00:00,  8.21it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 10.72it/s]\n",
            "                   all         90         90      0.869      0.778      0.908      0.372\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      28/99       1.2G    0.04568    0.01098   0.005345          4        640: 100% 53/53 [00:06<00:00,  8.38it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 10.29it/s]\n",
            "                   all         90         90      0.794      0.808       0.87      0.378\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      29/99       1.2G    0.04387    0.01116   0.005231          5        640: 100% 53/53 [00:06<00:00,  8.20it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.37it/s]\n",
            "                   all         90         90       0.89      0.859       0.93      0.418\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      30/99       1.2G    0.04449    0.01038   0.004939          5        640: 100% 53/53 [00:06<00:00,  8.29it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.53it/s]\n",
            "                   all         90         90      0.854      0.827      0.884      0.401\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      31/99       1.2G    0.04274    0.01077   0.003965          1        640: 100% 53/53 [00:06<00:00,  7.82it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.71it/s]\n",
            "                   all         90         90      0.944      0.893      0.938      0.475\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      32/99       1.2G    0.04239    0.01152   0.006818          2        640: 100% 53/53 [00:06<00:00,  8.30it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00,  9.89it/s]\n",
            "                   all         90         90      0.906      0.865      0.929      0.412\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      33/99       1.2G      0.042   0.009675   0.003921          3        640: 100% 53/53 [00:06<00:00,  8.52it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 10.77it/s]\n",
            "                   all         90         90      0.758      0.904      0.907      0.439\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      34/99       1.2G    0.03989    0.01004   0.004426          3        640: 100% 53/53 [00:06<00:00,  8.46it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.65it/s]\n",
            "                   all         90         90      0.866      0.857      0.906      0.388\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      35/99       1.2G    0.04091    0.01015   0.005485          2        640: 100% 53/53 [00:06<00:00,  7.99it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.30it/s]\n",
            "                   all         90         90      0.906      0.885      0.938       0.42\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      36/99       1.2G    0.04179    0.01005   0.004189          5        640: 100% 53/53 [00:06<00:00,  8.36it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.84it/s]\n",
            "                   all         90         90      0.857      0.911      0.941      0.469\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      37/99       1.2G    0.03934     0.0103   0.004732          3        640: 100% 53/53 [00:06<00:00,  8.12it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.49it/s]\n",
            "                   all         90         90      0.874      0.853      0.954      0.465\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      38/99       1.2G    0.03923    0.01019   0.004985          3        640: 100% 53/53 [00:08<00:00,  5.91it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00,  6.05it/s]\n",
            "                   all         90         90      0.847      0.841      0.886      0.439\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      39/99       1.2G    0.04156    0.01118   0.005075          3        640: 100% 53/53 [00:06<00:00,  8.10it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.82it/s]\n",
            "                   all         90         90      0.919      0.866      0.941       0.45\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      40/99       1.2G     0.0378   0.009045   0.004028          1        640: 100% 53/53 [00:06<00:00,  8.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.92it/s]\n",
            "                   all         90         90      0.897       0.91      0.961       0.43\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      41/99       1.2G    0.03936    0.01004   0.004059          2        640: 100% 53/53 [00:06<00:00,  8.44it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.80it/s]\n",
            "                   all         90         90      0.933      0.908      0.954      0.449\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      42/99       1.2G    0.03927   0.009862    0.00362          2        640: 100% 53/53 [00:06<00:00,  8.38it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:00<00:00, 12.09it/s]\n",
            "                   all         90         90      0.923      0.949      0.973      0.472\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      43/99       1.2G    0.03629   0.009207    0.00524          2        640: 100% 53/53 [00:06<00:00,  8.43it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:00<00:00, 12.07it/s]\n",
            "                   all         90         90      0.933      0.893      0.961      0.433\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      44/99       1.2G    0.03956   0.009392   0.003362          4        640: 100% 53/53 [00:06<00:00,  8.41it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.66it/s]\n",
            "                   all         90         90      0.843       0.85      0.905      0.398\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      45/99       1.2G    0.03952    0.01019   0.003165          2        640: 100% 53/53 [00:06<00:00,  8.41it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:00<00:00, 12.06it/s]\n",
            "                   all         90         90      0.954      0.908      0.973      0.471\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      46/99       1.2G    0.03812    0.01019   0.004222          3        640: 100% 53/53 [00:06<00:00,  8.44it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.89it/s]\n",
            "                   all         90         90      0.881      0.856      0.921      0.436\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      47/99       1.2G     0.0381    0.01027   0.003602          6        640: 100% 53/53 [00:06<00:00,  8.19it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.85it/s]\n",
            "                   all         90         90      0.857      0.935       0.95      0.484\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      48/99       1.2G    0.03725    0.01013   0.003601          1        640: 100% 53/53 [00:06<00:00,  8.50it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:00<00:00, 12.10it/s]\n",
            "                   all         90         90      0.947      0.916      0.949      0.449\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      49/99       1.2G    0.03561    0.01011   0.002505          5        640: 100% 53/53 [00:06<00:00,  8.54it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.89it/s]\n",
            "                   all         90         90      0.912      0.915      0.953      0.445\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      50/99       1.2G    0.03603   0.009119   0.003269          2        640: 100% 53/53 [00:06<00:00,  8.36it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:00<00:00, 12.06it/s]\n",
            "                   all         90         90      0.947      0.933      0.967      0.451\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      51/99       1.2G    0.03524   0.008585   0.002315          2        640: 100% 53/53 [00:06<00:00,  8.40it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.99it/s]\n",
            "                   all         90         90      0.913      0.896      0.951      0.436\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      52/99       1.2G    0.03648   0.009779   0.002291          3        640: 100% 53/53 [00:06<00:00,  8.43it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.90it/s]\n",
            "                   all         90         90      0.853      0.862      0.902      0.381\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      53/99       1.2G    0.03513   0.009262   0.002973          5        640: 100% 53/53 [00:06<00:00,  8.50it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.99it/s]\n",
            "                   all         90         90      0.932      0.904      0.934      0.458\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      54/99       1.2G    0.03511   0.009734   0.003237          1        640: 100% 53/53 [00:06<00:00,  8.13it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.41it/s]\n",
            "                   all         90         90      0.945      0.919      0.968      0.508\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      55/99       1.2G    0.03112   0.008251   0.002167          2        640: 100% 53/53 [00:06<00:00,  8.37it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00,  9.38it/s]\n",
            "                   all         90         90       0.91      0.947      0.966      0.481\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      56/99       1.2G    0.03338   0.009051    0.00232          3        640: 100% 53/53 [00:06<00:00,  8.40it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 10.29it/s]\n",
            "                   all         90         90      0.972      0.923      0.972      0.503\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      57/99       1.2G    0.03515   0.009034   0.003846          1        640: 100% 53/53 [00:06<00:00,  8.20it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:00<00:00, 12.00it/s]\n",
            "                   all         90         90      0.853      0.918      0.933      0.464\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      58/99       1.2G    0.03337   0.008763   0.002539          2        640: 100% 53/53 [00:06<00:00,  8.41it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.50it/s]\n",
            "                   all         90         90      0.899      0.927      0.952      0.469\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      59/99       1.2G    0.03412   0.008647   0.001766          2        640: 100% 53/53 [00:06<00:00,  8.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.87it/s]\n",
            "                   all         90         90      0.931      0.924       0.95      0.462\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      60/99       1.2G     0.0319   0.008092   0.002589          2        640: 100% 53/53 [00:06<00:00,  8.35it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.85it/s]\n",
            "                   all         90         90       0.95      0.935      0.967      0.492\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      61/99       1.2G    0.03222   0.008889   0.002276          4        640: 100% 53/53 [00:06<00:00,  8.33it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.76it/s]\n",
            "                   all         90         90      0.967      0.943      0.968      0.495\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      62/99       1.2G    0.03298   0.009774   0.002814          3        640: 100% 53/53 [00:06<00:00,  8.46it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.70it/s]\n",
            "                   all         90         90      0.953       0.95      0.959      0.481\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      63/99       1.2G    0.03061    0.00899   0.002163          4        640: 100% 53/53 [00:06<00:00,  8.47it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.66it/s]\n",
            "                   all         90         90      0.984      0.949      0.977       0.51\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      64/99       1.2G    0.03137   0.008705   0.002611          2        640: 100% 53/53 [00:06<00:00,  8.51it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.41it/s]\n",
            "                   all         90         90      0.963       0.94      0.967      0.486\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      65/99       1.2G    0.03204   0.008807   0.003417          2        640: 100% 53/53 [00:06<00:00,  8.44it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.79it/s]\n",
            "                   all         90         90      0.958      0.926      0.947      0.472\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      66/99       1.2G    0.03165    0.00843   0.002408          3        640: 100% 53/53 [00:06<00:00,  8.23it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.62it/s]\n",
            "                   all         90         90      0.993      0.929      0.976       0.48\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      67/99       1.2G    0.03014   0.008827   0.003126          3        640: 100% 53/53 [00:06<00:00,  8.32it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 10.34it/s]\n",
            "                   all         90         90      0.981       0.92      0.974      0.483\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      68/99       1.2G    0.03116   0.008294   0.002277          8        640: 100% 53/53 [00:06<00:00,  8.40it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.85it/s]\n",
            "                   all         90         90      0.961      0.968      0.982      0.503\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      69/99       1.2G    0.03032   0.008421   0.001711          3        640: 100% 53/53 [00:06<00:00,  8.29it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.88it/s]\n",
            "                   all         90         90      0.969      0.946      0.976      0.504\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      70/99       1.2G     0.0299   0.007973   0.001496          1        640: 100% 53/53 [00:06<00:00,  8.47it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.84it/s]\n",
            "                   all         90         90      0.966      0.904      0.977      0.483\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      71/99       1.2G    0.02956   0.008235   0.002981          8        640: 100% 53/53 [00:06<00:00,  8.52it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.95it/s]\n",
            "                   all         90         90      0.935      0.967      0.975      0.499\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      72/99       1.2G    0.03113   0.007905   0.002151          2        640: 100% 53/53 [00:07<00:00,  7.22it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00,  6.60it/s]\n",
            "                   all         90         90      0.946      0.936      0.968      0.483\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      73/99       1.2G    0.03042   0.008135   0.002723          1        640: 100% 53/53 [00:08<00:00,  6.61it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.88it/s]\n",
            "                   all         90         90      0.937      0.939      0.973      0.494\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      74/99       1.2G    0.02852   0.007491    0.00263          4        640: 100% 53/53 [00:06<00:00,  8.41it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.79it/s]\n",
            "                   all         90         90      0.944      0.976      0.983      0.471\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      75/99       1.2G     0.0291    0.00819   0.001556          4        640: 100% 53/53 [00:06<00:00,  8.46it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.50it/s]\n",
            "                   all         90         90      0.953      0.961      0.969      0.492\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      76/99       1.2G    0.02748   0.007986   0.001575          2        640: 100% 53/53 [00:06<00:00,  8.36it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.84it/s]\n",
            "                   all         90         90      0.949      0.965      0.964       0.48\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      77/99       1.2G    0.02807   0.007842   0.001913          2        640: 100% 53/53 [00:06<00:00,  8.29it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.85it/s]\n",
            "                   all         90         90      0.976      0.986      0.988      0.509\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      78/99       1.2G    0.02721   0.008035   0.001618          2        640: 100% 53/53 [00:06<00:00,  8.37it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.73it/s]\n",
            "                   all         90         90      0.968      0.972       0.97      0.503\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      79/99       1.2G    0.02822   0.008413   0.002025          7        640: 100% 53/53 [00:06<00:00,  8.29it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.39it/s]\n",
            "                   all         90         90      0.969      0.968      0.971      0.497\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      80/99       1.2G    0.02616   0.008121   0.001624          3        640: 100% 53/53 [00:06<00:00,  8.50it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.94it/s]\n",
            "                   all         90         90      0.955      0.971      0.973      0.487\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      81/99       1.2G     0.0275   0.008069   0.001726          7        640: 100% 53/53 [00:06<00:00,  8.24it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 10.32it/s]\n",
            "                   all         90         90      0.953      0.959      0.971      0.486\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      82/99       1.2G     0.0272   0.008093    0.00275          2        640: 100% 53/53 [00:06<00:00,  8.06it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.27it/s]\n",
            "                   all         90         90      0.939      0.976      0.976       0.51\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      83/99       1.2G    0.02568   0.007787   0.001837          3        640: 100% 53/53 [00:06<00:00,  8.48it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00,  9.70it/s]\n",
            "                   all         90         90      0.953      0.974      0.976      0.489\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      84/99       1.2G    0.02728   0.007982   0.001261          4        640: 100% 53/53 [00:06<00:00,  8.37it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.95it/s]\n",
            "                   all         90         90      0.942      0.969      0.972      0.481\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      85/99       1.2G    0.02576   0.007507   0.001174          3        640: 100% 53/53 [00:06<00:00,  8.38it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 10.44it/s]\n",
            "                   all         90         90      0.958       0.96      0.972      0.491\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      86/99       1.2G    0.02569     0.0074  0.0009596          4        640: 100% 53/53 [00:06<00:00,  8.32it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.99it/s]\n",
            "                   all         90         90      0.974      0.966      0.985      0.495\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      87/99       1.2G     0.0262   0.008125   0.002482          4        640: 100% 53/53 [00:06<00:00,  8.36it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.77it/s]\n",
            "                   all         90         90      0.942      0.975      0.983      0.489\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      88/99       1.2G    0.02467   0.007295    0.00131          5        640: 100% 53/53 [00:06<00:00,  8.53it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.92it/s]\n",
            "                   all         90         90      0.938      0.974      0.983       0.48\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      89/99       1.2G    0.02516   0.007521   0.002199          2        640: 100% 53/53 [00:06<00:00,  8.33it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.75it/s]\n",
            "                   all         90         90      0.963      0.961      0.985      0.494\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      90/99       1.2G    0.02502   0.007694   0.001219          5        640: 100% 53/53 [00:06<00:00,  8.45it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:00<00:00, 12.10it/s]\n",
            "                   all         90         90      0.971      0.961      0.984      0.493\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      91/99       1.2G    0.02451   0.007092   0.001249          5        640: 100% 53/53 [00:06<00:00,  8.35it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.86it/s]\n",
            "                   all         90         90      0.938      0.978      0.983       0.49\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      92/99       1.2G    0.02634   0.008107   0.001606          4        640: 100% 53/53 [00:06<00:00,  8.44it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.84it/s]\n",
            "                   all         90         90      0.969       0.97      0.985      0.498\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      93/99       1.2G    0.02469   0.008339   0.001559          4        640: 100% 53/53 [00:06<00:00,  8.35it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:00<00:00, 12.02it/s]\n",
            "                   all         90         90       0.97      0.972      0.986      0.496\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      94/99       1.2G    0.02426    0.00788   0.001113          5        640: 100% 53/53 [00:06<00:00,  8.58it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:00<00:00, 12.21it/s]\n",
            "                   all         90         90      0.984      0.973      0.986      0.505\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      95/99       1.2G    0.02432   0.007127  0.0009223          3        640: 100% 53/53 [00:06<00:00,  8.42it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.86it/s]\n",
            "                   all         90         90      0.972      0.973      0.986      0.504\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      96/99       1.2G    0.02505   0.007504   0.001678          1        640: 100% 53/53 [00:06<00:00,  8.45it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.96it/s]\n",
            "                   all         90         90      0.972      0.969      0.986      0.515\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      97/99       1.2G    0.02387   0.007224   0.001268          5        640: 100% 53/53 [00:06<00:00,  8.42it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:00<00:00, 12.04it/s]\n",
            "                   all         90         90      0.951      0.965      0.966      0.495\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      98/99       1.2G    0.02369   0.007688   0.001335          2        640: 100% 53/53 [00:06<00:00,  8.08it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.85it/s]\n",
            "                   all         90         90      0.959      0.972      0.977      0.491\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      99/99       1.2G    0.02319   0.007773   0.001395          2        640: 100% 53/53 [00:06<00:00,  8.27it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:01<00:00, 11.00it/s]\n",
            "                   all         90         90       0.96      0.972      0.977      0.497\n",
            "\n",
            "100 epochs completed in 0.224 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/alwani/weights/last.pt, 14.4MB\n",
            "Optimizer stripped from /content/drive/MyDrive/alwani/weights/best.pt, 14.4MB\n",
            "\n",
            "Validating /content/drive/MyDrive/alwani/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 12/12 [00:02<00:00,  5.07it/s]\n",
            "                   all         90         90      0.973      0.969      0.986      0.514\n",
            "                 peace         90         26      0.989          1      0.995      0.579\n",
            "                 shaka         90         26      0.962      0.987      0.994      0.479\n",
            "                thumbs         90         38      0.967      0.921      0.968      0.483\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/alwani\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python val.py --weights /content/drive/MyDrive/alwani/weights/alwani.pt --data {dataset.location}/data.yaml --img 640 --project /content/drive/MyDrive --name val_alwani"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiaMckZMBxIv",
        "outputId": "331b793f-e68e-4be4-bf8b-827086b0cd67"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file 'val.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --img 800 --weights /content/drive/MyDrive/alwani/weights/alwani.pt --project /content/drive/MyDrive/yoloDetect --name peace_detect --source /content/drive/MyDrive/yoloDetect/peace.mp4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1YnDcroHFOS",
        "outputId": "560d7d79-597f-4387-c342-f07c2dbc949c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/alwani/weights/alwani.pt'], source=/content/drive/MyDrive/yoloDetect/peace.mp4, data=data/coco128.yaml, imgsz=[800, 800], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=/content/drive/MyDrive/yoloDetect, name=peace_detect, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5  v7.0-32-g357cde9 Python-3.8.16 torch-1.13.0+cu116 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
            "video 1/1 (1/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 (no detections), 12.1ms\n",
            "video 1/1 (2/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 12.0ms\n",
            "video 1/1 (3/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 12.0ms\n",
            "video 1/1 (4/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 11.9ms\n",
            "video 1/1 (5/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 12.0ms\n",
            "video 1/1 (6/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 11.9ms\n",
            "video 1/1 (7/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 11.9ms\n",
            "video 1/1 (8/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 11.9ms\n",
            "video 1/1 (9/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 11.9ms\n",
            "video 1/1 (10/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 12.0ms\n",
            "video 1/1 (11/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 10.7ms\n",
            "video 1/1 (12/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 9.0ms\n",
            "video 1/1 (13/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 9.6ms\n",
            "video 1/1 (14/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 9.0ms\n",
            "video 1/1 (15/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 9.0ms\n",
            "video 1/1 (16/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 9.0ms\n",
            "video 1/1 (17/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 8.9ms\n",
            "video 1/1 (18/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 9.0ms\n",
            "video 1/1 (19/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 9.0ms\n",
            "video 1/1 (20/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 9.0ms\n",
            "video 1/1 (21/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 9.5ms\n",
            "video 1/1 (22/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 9.0ms\n",
            "video 1/1 (23/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.9ms\n",
            "video 1/1 (24/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.5ms\n",
            "video 1/1 (25/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.6ms\n",
            "video 1/1 (26/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.8ms\n",
            "video 1/1 (27/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.6ms\n",
            "video 1/1 (28/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.9ms\n",
            "video 1/1 (29/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.7ms\n",
            "video 1/1 (30/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 10.9ms\n",
            "video 1/1 (31/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.5ms\n",
            "video 1/1 (32/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.7ms\n",
            "video 1/1 (33/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.6ms\n",
            "video 1/1 (34/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.7ms\n",
            "video 1/1 (35/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 8.0ms\n",
            "video 1/1 (36/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.9ms\n",
            "video 1/1 (37/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.9ms\n",
            "video 1/1 (38/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 8.0ms\n",
            "video 1/1 (39/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 12.6ms\n",
            "video 1/1 (40/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 8.1ms\n",
            "video 1/1 (41/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.8ms\n",
            "video 1/1 (42/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 8.1ms\n",
            "video 1/1 (43/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.7ms\n",
            "video 1/1 (44/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.9ms\n",
            "video 1/1 (45/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.7ms\n",
            "video 1/1 (46/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.6ms\n",
            "video 1/1 (47/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.8ms\n",
            "video 1/1 (48/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.5ms\n",
            "video 1/1 (49/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.7ms\n",
            "video 1/1 (50/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 7.7ms\n",
            "video 1/1 (51/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 8.2ms\n",
            "video 1/1 (52/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 12.8ms\n",
            "video 1/1 (53/53) /content/drive/MyDrive/yoloDetect/peace.mp4: 800x448 1 peace, 8.0ms\n",
            "Speed: 0.5ms pre-process, 9.1ms inference, 0.9ms NMS per image at shape (1, 3, 800, 800)\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/yoloDetect/peace_detect\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --img 800 --weights /content/drive/MyDrive/alwani/weights/alwani.pt --project /content/drive/MyDrive/yoloDetect --name thumbs_detect --source /content/drive/MyDrive/yoloDetect/thumbs.mp4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "097f88f8-8709-44af-d427-82447603f74b",
        "id": "Hc9UsyhiOzlX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/alwani/weights/alwani.pt'], source=/content/drive/MyDrive/yoloDetect/thumbs.mp4, data=data/coco128.yaml, imgsz=[800, 800], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=/content/drive/MyDrive/yoloDetect, name=thumbs_detect, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5  v7.0-32-g357cde9 Python-3.8.16 torch-1.13.0+cu116 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
            "video 1/1 (1/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 12.1ms\n",
            "video 1/1 (2/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 11.9ms\n",
            "video 1/1 (3/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 shaka, 1 thumbs, 11.9ms\n",
            "video 1/1 (4/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 11.9ms\n",
            "video 1/1 (5/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 11.9ms\n",
            "video 1/1 (6/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 11.9ms\n",
            "video 1/1 (7/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 11.9ms\n",
            "video 1/1 (8/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 11.9ms\n",
            "video 1/1 (9/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 11.9ms\n",
            "video 1/1 (10/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 11.9ms\n",
            "video 1/1 (11/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 11.9ms\n",
            "video 1/1 (12/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 11.9ms\n",
            "video 1/1 (13/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 12.0ms\n",
            "video 1/1 (14/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 9.2ms\n",
            "video 1/1 (15/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 8.6ms\n",
            "video 1/1 (16/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 8.6ms\n",
            "video 1/1 (17/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 8.6ms\n",
            "video 1/1 (18/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 8.6ms\n",
            "video 1/1 (19/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 8.6ms\n",
            "video 1/1 (20/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 8.6ms\n",
            "video 1/1 (21/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 8.6ms\n",
            "video 1/1 (22/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 8.6ms\n",
            "video 1/1 (23/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 8.6ms\n",
            "video 1/1 (24/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 8.6ms\n",
            "video 1/1 (25/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 7.8ms\n",
            "video 1/1 (26/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 7.5ms\n",
            "video 1/1 (27/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 7.5ms\n",
            "video 1/1 (28/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 7.4ms\n",
            "video 1/1 (29/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 7.4ms\n",
            "video 1/1 (30/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 11.2ms\n",
            "video 1/1 (31/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 7.8ms\n",
            "video 1/1 (32/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 7.8ms\n",
            "video 1/1 (33/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 7.5ms\n",
            "video 1/1 (34/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 7.7ms\n",
            "video 1/1 (35/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 7.5ms\n",
            "video 1/1 (36/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 8.0ms\n",
            "video 1/1 (37/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 7.5ms\n",
            "video 1/1 (38/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 7.6ms\n",
            "video 1/1 (39/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 7.3ms\n",
            "video 1/1 (40/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 7.2ms\n",
            "video 1/1 (41/41) /content/drive/MyDrive/yoloDetect/thumbs.mp4: 800x448 1 thumbs, 7.3ms\n",
            "Speed: 0.5ms pre-process, 9.3ms inference, 0.9ms NMS per image at shape (1, 3, 800, 800)\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/yoloDetect/thumbs_detect\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --img 800 --weights /content/drive/MyDrive/alwani/weights/alwani.pt --project /content/drive/MyDrive/yoloDetect --name shaka_detect --source /content/drive/MyDrive/yoloDetect/shaka.mp4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56def38-004d-4692-ddde-04ba8eea741f",
        "id": "2ydDvRZ0QGR5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/alwani/weights/alwani.pt'], source=/content/drive/MyDrive/yoloDetect/shaka.mp4, data=data/coco128.yaml, imgsz=[800, 800], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=/content/drive/MyDrive/yoloDetect, name=shaka_detect, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5  v7.0-32-g357cde9 Python-3.8.16 torch-1.13.0+cu116 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
            "video 1/1 (1/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 thumbs, 12.3ms\n",
            "video 1/1 (2/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 thumbs, 12.1ms\n",
            "video 1/1 (3/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 thumbs, 12.1ms\n",
            "video 1/1 (4/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 1 thumbs, 12.1ms\n",
            "video 1/1 (5/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 1 thumbs, 12.1ms\n",
            "video 1/1 (6/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 12.0ms\n",
            "video 1/1 (7/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 12.0ms\n",
            "video 1/1 (8/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 11.9ms\n",
            "video 1/1 (9/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 11.9ms\n",
            "video 1/1 (10/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 11.9ms\n",
            "video 1/1 (11/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 9.5ms\n",
            "video 1/1 (12/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 9.6ms\n",
            "video 1/1 (13/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 9.6ms\n",
            "video 1/1 (14/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 9.6ms\n",
            "video 1/1 (15/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 9.6ms\n",
            "video 1/1 (16/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 9.6ms\n",
            "video 1/1 (17/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 9.6ms\n",
            "video 1/1 (18/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 9.6ms\n",
            "video 1/1 (19/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 9.5ms\n",
            "video 1/1 (20/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 9.6ms\n",
            "video 1/1 (21/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 9.9ms\n",
            "video 1/1 (22/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.5ms\n",
            "video 1/1 (23/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 9.0ms\n",
            "video 1/1 (24/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 10.3ms\n",
            "video 1/1 (25/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.0ms\n",
            "video 1/1 (26/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.3ms\n",
            "video 1/1 (27/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 7.9ms\n",
            "video 1/1 (28/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.2ms\n",
            "video 1/1 (29/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.4ms\n",
            "video 1/1 (30/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.5ms\n",
            "video 1/1 (31/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.2ms\n",
            "video 1/1 (32/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 10.2ms\n",
            "video 1/1 (33/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.1ms\n",
            "video 1/1 (34/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 9.0ms\n",
            "video 1/1 (35/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.4ms\n",
            "video 1/1 (36/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.0ms\n",
            "video 1/1 (37/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.4ms\n",
            "video 1/1 (38/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.9ms\n",
            "video 1/1 (39/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 13.5ms\n",
            "video 1/1 (40/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 18.4ms\n",
            "video 1/1 (41/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 15.8ms\n",
            "video 1/1 (42/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.4ms\n",
            "video 1/1 (43/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.1ms\n",
            "video 1/1 (44/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.4ms\n",
            "video 1/1 (45/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 9.2ms\n",
            "video 1/1 (46/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.8ms\n",
            "video 1/1 (47/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.7ms\n",
            "video 1/1 (48/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 7.8ms\n",
            "video 1/1 (49/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.0ms\n",
            "video 1/1 (50/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.9ms\n",
            "video 1/1 (51/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.9ms\n",
            "video 1/1 (52/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 12.3ms\n",
            "video 1/1 (53/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 10.1ms\n",
            "video 1/1 (54/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.3ms\n",
            "video 1/1 (55/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.1ms\n",
            "video 1/1 (56/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 12.0ms\n",
            "video 1/1 (57/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 9.5ms\n",
            "video 1/1 (58/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 9.2ms\n",
            "video 1/1 (59/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 9.5ms\n",
            "video 1/1 (60/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.5ms\n",
            "video 1/1 (61/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 9.0ms\n",
            "video 1/1 (62/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.8ms\n",
            "video 1/1 (63/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.4ms\n",
            "video 1/1 (64/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.5ms\n",
            "video 1/1 (65/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 7.7ms\n",
            "video 1/1 (66/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.0ms\n",
            "video 1/1 (67/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.6ms\n",
            "video 1/1 (68/68) /content/drive/MyDrive/yoloDetect/shaka.mp4: 800x448 1 shaka, 8.7ms\n",
            "Speed: 0.5ms pre-process, 9.7ms inference, 1.6ms NMS per image at shape (1, 3, 800, 800)\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/yoloDetect/shaka_detect\u001b[0m\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f4e43933d5ad476583d7eee0d9dc09be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d9ded1fd76f44fea1408fdf65be08f3",
              "IPY_MODEL_0373f147e91a4b778bed454de012a248",
              "IPY_MODEL_167ea6a856244d3b9907242c081c9bf1"
            ],
            "layout": "IPY_MODEL_4a12f6f596bb49cfa501365fdea97c70"
          }
        },
        "4d9ded1fd76f44fea1408fdf65be08f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d76347c85b6c4dc2963f6c274a279772",
            "placeholder": "",
            "style": "IPY_MODEL_5964e9a766db4c70b872205b5403b45d",
            "value": "100%"
          }
        },
        "0373f147e91a4b778bed454de012a248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14ba82b2989847939afe4128ee9c2672",
            "max": 52147035,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d202564baa6648898296e66ffea54b90",
            "value": 52147035
          }
        },
        "167ea6a856244d3b9907242c081c9bf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3488e2b0f21f44868b16fc1909ef8ab4",
            "placeholder": "",
            "style": "IPY_MODEL_a3710d75470c45d19a47db5d0cff320a",
            "value": " 49.7M/49.7M [00:00&lt;00:00, 239MB/s]"
          }
        },
        "4a12f6f596bb49cfa501365fdea97c70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d76347c85b6c4dc2963f6c274a279772": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5964e9a766db4c70b872205b5403b45d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14ba82b2989847939afe4128ee9c2672": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d202564baa6648898296e66ffea54b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3488e2b0f21f44868b16fc1909ef8ab4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3710d75470c45d19a47db5d0cff320a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}